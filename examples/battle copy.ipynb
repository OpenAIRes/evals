{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BATTLE Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"dataset\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "choices = \"user\"                     # options: user, ai\n",
    "num_few_shot = 4\n",
    "initial_message = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "roles = \"user\"                      # options: system-user, system, user\n",
    "battles_generation = \"dense\"        # options: dense, only-new\n",
    "\n",
    "#!cd evals\n",
    "#!git lfs fetch --all\n",
    "#!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "client = OpenAI()\n",
    "#import datetime\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "#api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Install Evals if you haven't already\n",
    "# %pip install -e ../.\n",
    "# pip install --upgrade openai\n",
    "# %pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths. Assuming this notebook is in examples/\n",
    "\n",
    "evals_path = os.path.join(os.getcwd(), \"..\", \"evals\")\n",
    "\n",
    "registry_path = os.path.join(evals_path, \"registry\", \"evals\", \"battles.yaml\")\n",
    "\n",
    "data_path = os.path.join(evals_path, \"registry\", \"data\", \"battles\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "data_path = os.path.join(data_path, \"samples.jsonl\")\n",
    "\n",
    "json_logs_path = os.path.join(os.getcwd(), \"logs\")\n",
    "os.makedirs(json_logs_path, exist_ok=True)\n",
    "json_logs_path = os.path.join(json_logs_path, \"logs\")\n",
    "\n",
    "df_path = os.path.join(evals_path, \"evallogs\", \"df\")\n",
    "os.makedirs(df_path, exist_ok=True)\n",
    "dataset_path = os.path.join(df_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registry yaml\n",
    "\n",
    "registry_yaml = {}\n",
    "\n",
    "registry_yaml[\"battles\"] = {\n",
    "    \"id\": \"battles.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"battles.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.modelgraded.classify:ModelBasedClassify\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"battles/samples.jsonl\",\n",
    "        \"eval_type\": \"cot_classify\",\n",
    "        \"modelgraded_spec\": \"battle\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(registry_path), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "try:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset = pd.read_json(f, lines=True)\n",
    "    if dataset.iloc[-1][\"Choice\"] == \"No\":\n",
    "        best_content = dataset.iloc[-1][\"Instruction2\"]\n",
    "        best_response = dataset.iloc[-1][\"Response2\"]\n",
    "    else:\n",
    "        best_content = dataset.iloc[-1][\"Instruction1\"]\n",
    "        best_response = dataset.iloc[-1][\"Response1\"]\n",
    "    best_message = [{\"role\": \"user\", \"content\": best_content}]\n",
    "\n",
    "# Or create new data file\n",
    "\n",
    "except FileNotFoundError:\n",
    "    dataset = []\n",
    "    dataset = pd.DataFrame(dataset, columns=['Instruction1', 'Response1', 'Instruction2', 'Response2', 'Sampled', 'Choice', 'Data'])\n",
    "    best_message = initial_message\n",
    "    best_response = client.chat.completions.create(\n",
    "        messages = initial_message,\n",
    "        model = model,\n",
    "        ).choices[0].message.content\n",
    "    \n",
    "candidate_messages = [best_message]\n",
    "next_candidate_messages = []\n",
    "generation_distance = 0\n",
    "message_in_candidate_messages = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(best_message, best_response, candidate_message):\n",
    "\n",
    "    dataset = [{\"input1\": candidate_message, \"input2\": best_message, \"completion2\":best_response}]\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(data_path, orient=\"records\", lines=True)\n",
    "\n",
    "    !oaieval gpt-3.5-turbo battles --record_path logs/logs\n",
    "    \n",
    "    with open(json_logs_path, \"r\") as f:\n",
    "        df = pd.read_json(f, lines=True)\n",
    "\n",
    "    #current_time = datetime.datetime.now()\n",
    "    #formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #df.to_json(os.path.join(df_path, formatted_time), lines=True, orient=\"records\")\n",
    "\n",
    "    instruction1 = candidate_message[-1][\"content\"]\n",
    "    instruction2 = best_message[-1][\"content\"]\n",
    "\n",
    "    battle_prompt_content = df[\"data\"].iloc[-2][\"prompt\"][0][\"content\"]\n",
    "    response1 = battle_prompt_content.split(\"\\n[Response 1]\\n\",)[1].split(\"\\n\\n[Instruction 2]\\n\")[0]\n",
    "    response1 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    response2 = battle_prompt_content.split(\"\\n[Response 2]\\n\",)[1].split(\"\\n\\n\\nIs the first response better than the second?\")[0]\n",
    "    response2 = response2.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    print(f\"Response1: {response1}\")\n",
    "    #print(f\"response2: {response2}\")\n",
    "\n",
    "    sampled = df[\"data\"].iloc[-2][\"sampled\"][0]\n",
    "\n",
    "    choice = df[\"data\"].iloc[-1][\"choice\"]\n",
    "\n",
    "    data = {'Instruction1': instruction1, 'Response1': response1, 'Instruction2': instruction2, 'Response2': response2, 'Sampled': sampled, 'Choice': choice, 'Data': {}}\n",
    "    data = pd.DataFrame([data])\n",
    "    data.at[0, \"Data\"] = df.to_dict()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_system_and_user(message):\n",
    "    new_message = []\n",
    "    if message[0][\"role\"] == \"system\":\n",
    "        new_message.append([{\"role\": \"user\", \"content\": message[0][\"content\"]}])\n",
    "    else:\n",
    "        new_message.append([{\"role\": \"system\", \"content\": message[0][\"content\"]}])\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(content, dataset):\n",
    "    #print (f\"finding parents {content}\")\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset = dataset[dataset[\"Response1\"] == content]\n",
    "    parents = dataset[\"Instruction1\"].unique()\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_children(content, dataset):\n",
    "    #print (f\"finding children {content}\")\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset = dataset[dataset[\"Instruction1\"] == content]\n",
    "    children = dataset[\"Response1\"].unique()\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_candidate_messages(dataset, best_message, generation_distance, roles):\n",
    "    best_content = best_message[-1][\"content\"]\n",
    "    last = [best_content]\n",
    "    list_of_contents = [best_content]\n",
    "\n",
    "    next = []\n",
    "    for i in range(generation_distance):\n",
    "        #print (f\"starting level {i}, generation distance {generation_distance}\")\n",
    "        for content in last:\n",
    "            #print(f\"last_up: {last_up}, now {content}\")\n",
    "            next.extend(find_parents(content, dataset))\n",
    "            next.extend(find_children(content, dataset))\n",
    "            \n",
    "        #print (f\"behind for loops\")\n",
    "        list_of_contents.extend(next)\n",
    "        last = next.copy()\n",
    "        #print(f\"endind level {i}\")\n",
    "        \n",
    "    list_of_contents = pd.array(list_of_contents).unique().tolist()\n",
    "    \n",
    "    #best_message_index = list_of_contents.index(best_message[0][content])\n",
    "    #start = max(best_message_index - generation_distance, 0)\n",
    "    #stop = min(len(list_of_contents))\n",
    "    #stop = len(list_of_contents) - 1\n",
    "\n",
    "    messages = []\n",
    "    for content in list_of_contents:\n",
    "        if roles == \"system-user\" or \"user\":\n",
    "            messages.append([{\"role\":\"user\",\"content\":content}])\n",
    "        if (roles == \"system-user\") or (roles == \"system\"):\n",
    "            print(\"Creating message without user role.\")\n",
    "            messages.append([{\"role\":\"system\",\"content\":content}])\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_shot_prompt(dataset,message,num_few_shot):\n",
    "    few_shot_prompt = []\n",
    "    for _, key_battle in dataset[dataset[\"Choice\"]==\"Yes\"].iloc[-num_few_shot:].iterrows():\n",
    "        few_shot_prompt.append({\"role\": \"system\", \"content\": key_battle[\"Instruction1\"], \"name\": \"example_user\"})\n",
    "        few_shot_prompt.append({\"role\": \"system\", \"content\": key_battle[\"Response1\"], \"name\": \"example_assistant\"})\n",
    "    message = few_shot_prompt + message\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are comparing two responses to the following two instructions.\n",
       "\n",
       "[Instruction 1]\n",
       "\n",
       "\n",
       "[Response 1]\n",
       "Hello! How can I assist you today?\n",
       "\n",
       "[Instruction 2]\n",
       "\n",
       "\n",
       "[Response 2]\n",
       "Hello! How can I assist you today?\n",
       "\n",
       "\n",
       "Is the first response better than the second? You must provide one answer based on your subjective view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if choices == \"user\":\n",
    "\n",
    "    prompt = \"You are comparing two responses to the following two instructions.\"\n",
    "\n",
    "    prompt += \"\\n\\n[Instruction 1]\\n\"\n",
    "    prompt += candidate_message[-1][\"content\"]\n",
    "\n",
    "    prompt += \"\\n\\n[Response 1]\\n\"\n",
    "    if message_in_candidate_messages <= len(candidate_messages):\n",
    "        candidate_message = candidate_messages[message_in_candidate_messages]\n",
    "        message_in_candidate_messages += 1\n",
    "    else:\n",
    "        if battles_generation == \"dense\":\n",
    "            candidate_messages = list_candidate_messages(dataset, best_message, generation_distance, roles)\n",
    "            generation_distance += 1\n",
    "            candidate_message = candidate_messages[0]\n",
    "            message_in_candidate_messages = 1\n",
    "            print(f\"candidate messages listed, generation distance: {generation_distance}, number of candidate messages: {len(candidate_messages)}\")\n",
    "        else:\n",
    "            print(\"Code not completed!\")\n",
    "    completion = client.chat.completions.create(\n",
    "        messages = candidate_message,\n",
    "        model = model,\n",
    "        )\n",
    "    response1 = completion.choices[0].message.content\n",
    "    prompt += response1\n",
    "    \n",
    "    prompt += \"\\n\\n[Instruction 2]\\n\"\n",
    "    prompt += best_message[-1][\"content\"]\n",
    "\n",
    "    prompt += \"\\n\\n[Response 2]\\n\"\n",
    "    prompt += best_response\n",
    "\n",
    "    prompt += \"\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\"\n",
    "\n",
    "    data = {'Instruction1': candidate_message[-1][\"content\"], 'Response1': response1, 'Instruction2': best_message[-1][\"content\"], 'Response2': best_response, 'Sampled': {}, 'Choice': \"\", 'Data': {}}\n",
    "    data = pd.DataFrame([data])\n",
    "    data.at[0, \"Data\"] = completion.to_dict()\n",
    "\n",
    "    display_markdown(prompt, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = \"No\"\n",
    "#choice = \"Yes\"\n",
    "data.at[0, \"Choice\"] = choice\n",
    "if choice == \"Yes\":\n",
    "    best_message = candidate_message\n",
    "    best_response = data[\"Response1\"].iloc[0]\n",
    "    print(f\"New best message and response!\")\n",
    "    if battles_generation == \"dense\":\n",
    "        generation_distance = 0\n",
    "dataset = pd.concat([dataset, data],ignore_index=True)\n",
    "dataset.to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instruction1</th>\n",
       "      <th>Response1</th>\n",
       "      <th>Instruction2</th>\n",
       "      <th>Response2</th>\n",
       "      <th>Sampled</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td></td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td></td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>{}</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Instruction1                           Response1 Instruction2  \\\n",
       "0               Hello! How can I assist you today?                \n",
       "1               Hello! How can I assist you today?                \n",
       "\n",
       "                            Response2 Sampled Choice Data  \n",
       "0  Hello! How can I assist you today?      {}          No  \n",
       "1  Hello! How can I assist you today?      {}     No   No  "
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if choices == \"ai\":\n",
    "    for candidate_message in candidate_messages:\n",
    "        if num_few_shot > 0:\n",
    "            candidate_message = make_x_shot_prompt(dataset,candidate_message,num_few_shot)\n",
    "        data = battle(best_message, best_response, candidate_message)\n",
    "        dataset = pd.concat([dataset, data],ignore_index=True)\n",
    "        dataset.to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "        if data[\"Choice\"].iloc[0] == \"Yes\":\n",
    "            best_message = candidate_message\n",
    "            best_response = data[\"Response1\"].iloc[0]\n",
    "            print(f\"New best message:{best_message}\")\n",
    "            if battles_generation == \"dense\":\n",
    "                generation_distance = 0\n",
    "\n",
    "        print(f\"Instruction1: {candidate_message[-1][\"content\"]}\")\n",
    "\n",
    "        if battles_generation == \"only-new\":\n",
    "            new_message = [{\"role\": candidate_message[0][\"role\"], \"content\":data[\"Response1\"].iloc[0]}]\n",
    "            if new_message not in next_candidate_messages:\n",
    "                next_candidate_messages.append(new_message)\n",
    "                if roles == \"system-user\":\n",
    "                    next_candidate_messages.append(switch_system_and_user(new_message))\n",
    "\n",
    "    if battles_generation == \"only-new\":\n",
    "        candidate_messages = next_candidate_messages\n",
    "        next_candidate_messages = []\n",
    "    if battles_generation == \"dense\":\n",
    "        #print(\"going to list candidate messages\")\n",
    "        candidate_messages = list_candidate_messages(dataset, best_message, generation_distance, roles)\n",
    "        generation_distance += 1\n",
    "        #print(\"candidate messages listed\")\n",
    "    print(f\"all done, generation distance: {generation_distance}, number of candidate messages: {len(candidate_messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dataset[dataset[\"Choice\"]==\"Yes\"].iloc[-1].drop(\"Data\"):\n",
    "    display_markdown(entry, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Sampled\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdbe172e46cfba2329a5e8d5b64cdf2d12f4dfd7d9bcea153ecef62d1d51933b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
