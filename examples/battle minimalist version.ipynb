{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "initial_message = [{\"role\": \"user\", \"content\": \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from IPython.display import display_markdown\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best(dataset):\n",
    "    if dataset.iloc[-1][\"Choice\"] == \"No\":\n",
    "        best_content = dataset.iloc[-1][\"Instruction2\"]\n",
    "        best_response = dataset.iloc[-1][\"Response2\"]\n",
    "    else:\n",
    "        best_content = dataset.iloc[-1][\"Instruction1\"]\n",
    "        best_response = dataset.iloc[-1][\"Response1\"]\n",
    "    best_message = [{\"role\": \"user\", \"content\": best_content}]\n",
    "    return best_message,best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(data_path, \"r\") as f:\n",
    "        dataset = pd.read_json(f, lines=True)\n",
    "    best_message, best_response = best(dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    dataset = []\n",
    "    dataset = pd.DataFrame(dataset, columns=['Instruction1', 'Response1', 'Instruction2', 'Response2', 'Sampled', 'Choice', 'Data'])\n",
    "    best_message = initial_message\n",
    "    best_response = client.chat.completions.create(\n",
    "        messages = initial_message,\n",
    "        model = model,\n",
    "        ).choices[0].message.content\n",
    "    \n",
    "candidate_messages = [best_message]\n",
    "next_candidate_messages = []\n",
    "generation_distance = 0\n",
    "message_in_candidate_messages = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(best_message, best_response, candidate_message):\n",
    "\n",
    "    dataset = [{\"input1\": candidate_message, \"input2\": best_message, \"completion2\":best_response}]\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(data_path, orient=\"records\", lines=True)\n",
    "\n",
    "    !oaieval gpt-3.5-turbo battles --record_path logs/logs\n",
    "    \n",
    "    with open(json_logs_path, \"r\") as f:\n",
    "        df = pd.read_json(f, lines=True)\n",
    "\n",
    "    #current_time = datetime.datetime.now()\n",
    "    #formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #df.to_json(os.path.join(df_path, formatted_time), lines=True, orient=\"records\")\n",
    "\n",
    "    instruction1 = candidate_message[-1][\"content\"]\n",
    "    instruction2 = best_message[-1][\"content\"]\n",
    "\n",
    "    battle_prompt_content = df[\"data\"].iloc[-2][\"prompt\"][0][\"content\"]\n",
    "    response1 = battle_prompt_content.split(\"\\n[Response 1]\\n\",)[1].split(\"\\n\\n[Instruction 2]\\n\")[0]\n",
    "    response1 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    response2 = battle_prompt_content.split(\"\\n[Response 2]\\n\",)[1].split(\"\\n\\n\\nIs the first response better than the second?\")[0]\n",
    "    response2 = response2.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    print(f\"Response1: {response1}\")\n",
    "    #print(f\"response2: {response2}\")\n",
    "\n",
    "    sampled = df[\"data\"].iloc[-2][\"sampled\"][0]\n",
    "\n",
    "    choice = df[\"data\"].iloc[-1][\"choice\"]\n",
    "\n",
    "    data = {'Instruction1': instruction1, 'Response1': response1, 'Instruction2': instruction2, 'Response2': response2, 'Sampled': sampled, 'Choice': choice, 'Data': {}}\n",
    "    data = pd.DataFrame([data])\n",
    "    data.at[0, \"Data\"] = df.to_dict()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_system_and_user(message):\n",
    "    new_message = []\n",
    "    if message[0][\"role\"] == \"system\":\n",
    "        new_message.append([{\"role\": \"user\", \"content\": message[0][\"content\"]}])\n",
    "    else:\n",
    "        new_message.append([{\"role\": \"system\", \"content\": message[0][\"content\"]}])\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(content, dataset):\n",
    "    #print (f\"finding parents {content}\")\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset = dataset[dataset[\"Response1\"] == content]\n",
    "    parents = dataset[\"Instruction1\"].unique()\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_children(content, dataset):\n",
    "    #print (f\"finding children {content}\")\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset = dataset[dataset[\"Instruction1\"] == content]\n",
    "    children = dataset[\"Response1\"].unique()\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_candidate_messages(dataset, best_message, generation_distance, roles):\n",
    "    best_content = best_message[-1][\"content\"]\n",
    "    last = [best_content]\n",
    "    list_of_contents = [best_content]\n",
    "\n",
    "    next = []\n",
    "    for i in range(generation_distance):\n",
    "        #print (f\"starting level {i}, generation distance {generation_distance}\")\n",
    "        for content in last:\n",
    "            #print(f\"last_up: {last_up}, now {content}\")\n",
    "            next.extend(find_parents(content, dataset))\n",
    "            next.extend(find_children(content, dataset))\n",
    "            \n",
    "        #print (f\"behind for loops\")\n",
    "        list_of_contents.extend(next)\n",
    "        last = next.copy()\n",
    "        #print(f\"endind level {i}\")\n",
    "        \n",
    "    list_of_contents = pd.array(list_of_contents).unique().tolist()\n",
    "    \n",
    "    #best_message_index = list_of_contents.index(best_message[0][content])\n",
    "    #start = max(best_message_index - generation_distance, 0)\n",
    "    #stop = min(len(list_of_contents))\n",
    "    #stop = len(list_of_contents) - 1\n",
    "\n",
    "    messages = []\n",
    "    for content in list_of_contents:\n",
    "        if roles == \"system-user\" or \"user\":\n",
    "            messages.append([{\"role\":\"user\",\"content\":content}])\n",
    "        if (roles == \"system-user\") or (roles == \"system\"):\n",
    "            print(\"Creating message without user role.\")\n",
    "            messages.append([{\"role\":\"system\",\"content\":content}])\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_shot_prompt(dataset,message,num_few_shot):\n",
    "    few_shot_prompt = []\n",
    "    for _, key_battle in dataset[dataset[\"Choice\"]==\"Yes\"].iloc[-num_few_shot:].iterrows():\n",
    "        few_shot_prompt.append({\"role\": \"system\", \"content\": key_battle[\"Instruction1\"], \"name\": \"example_user\"})\n",
    "        few_shot_prompt.append({\"role\": \"system\", \"content\": key_battle[\"Response1\"], \"name\": \"example_assistant\"})\n",
    "    message = few_shot_prompt + message\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_choice_prompt(candidate_message,candidate_messages,message_in_candidate_messages,dataset, best_message, best_response, generation_distance, roles, battles_generation):\n",
    "\n",
    "    prompt = \"You are comparing two responses to the following two instructions.\"\n",
    "\n",
    "    prompt += \"\\n\\n[Instruction 1]\\n\"\n",
    "    prompt += candidate_message[-1][\"content\"]\n",
    "\n",
    "    prompt += \"\\n\\n[Response 1]\\n\"\n",
    "    if message_in_candidate_messages < len(candidate_messages):\n",
    "        candidate_message = candidate_messages[message_in_candidate_messages]\n",
    "        message_in_candidate_messages += 1\n",
    "    else:\n",
    "        if battles_generation == \"dense\":\n",
    "            candidate_messages = list_candidate_messages(dataset, best_message, generation_distance, roles)\n",
    "            generation_distance += 1\n",
    "            candidate_message = candidate_messages[0]\n",
    "            message_in_candidate_messages = 1\n",
    "            print(f\"candidate messages listed, generation distance: {generation_distance}, number of candidate messages: {len(candidate_messages)}\")\n",
    "        else:\n",
    "            print(\"Code not completed!\")\n",
    "    completion = client.chat.completions.create(\n",
    "        messages = candidate_message,\n",
    "        model = model,\n",
    "        )\n",
    "    response1 = completion.choices[0].message.content\n",
    "    prompt += response1\n",
    "    \n",
    "    prompt += \"\\n\\n[Instruction 2]\\n\"\n",
    "    prompt += best_message[-1][\"content\"]\n",
    "\n",
    "    prompt += \"\\n\\n[Response 2]\\n\"\n",
    "    prompt += best_response\n",
    "\n",
    "    prompt += \"\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\"\n",
    "\n",
    "    data = {'Instruction1': candidate_message[-1][\"content\"], 'Response1': response1, 'Instruction2': best_message[-1][\"content\"], 'Response2': best_response, 'Sampled': {}, 'Choice': \"\", 'Data': {}}\n",
    "    data = pd.DataFrame([data])\n",
    "    data.at[0, \"Data\"] = completion.to_dict()\n",
    "\n",
    "    display_markdown(prompt, raw=True)\n",
    "    return data,candidate_messages,message_in_candidate_messages,generation_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are comparing two responses to the following two instructions.\n",
       "\n",
       "[Instruction 1]\n",
       "\n",
       "\n",
       "[Response 1]\n",
       "Hello! How can I assist you today?\n",
       "\n",
       "[Instruction 2]\n",
       "\n",
       "\n",
       "[Response 2]\n",
       "Hello! How can I assist you today?\n",
       "\n",
       "\n",
       "Is the first response better than the second? You must provide one answer based on your subjective view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if choices == \"user\":\n",
    "    data,candidate_messages,message_in_candidate_messages,generation_distance = user_choice_prompt(candidate_message,candidate_messages,message_in_candidate_messages,dataset, best_message, best_response, generation_distance, roles, battles_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer:No\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "user_choice_prompt() missing 1 required positional argument: 'battles_generation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1328], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([dataset, data],ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m dataset\u001b[38;5;241m.\u001b[39mto_json(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(df_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m user_choice_prompt(candidate_message,candidate_messages,message_in_candidate_messages,dataset, best_message, generation_distance, roles, battles_generation)\n",
      "\u001b[0;31mTypeError\u001b[0m: user_choice_prompt() missing 1 required positional argument: 'battles_generation'"
     ]
    }
   ],
   "source": [
    "choice = \"No\"\n",
    "#choice = \"Yes\"\n",
    "data.at[0, \"Choice\"] = choice\n",
    "print(f\"Your answer:{choice}\")\n",
    "if choice == \"Yes\":\n",
    "    best_message = candidate_message\n",
    "    best_response = data[\"Response1\"].iloc[0]\n",
    "    print(f\"New best message and response!\")\n",
    "    if battles_generation == \"dense\":\n",
    "        generation_distance = 0\n",
    "dataset = pd.concat([dataset, data],ignore_index=True)\n",
    "dataset.to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "data,candidate_messages,message_in_candidate_messages,generation_distance = user_choice_prompt(candidate_message,candidate_messages,message_in_candidate_messages,dataset, best_message, best_response, generation_distance, roles, battles_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if choices == \"ai\":\n",
    "    for candidate_message in candidate_messages:\n",
    "        if num_few_shot > 0:\n",
    "            candidate_message = make_x_shot_prompt(dataset,candidate_message,num_few_shot)\n",
    "        data = battle(best_message, best_response, candidate_message)\n",
    "        dataset = pd.concat([dataset, data],ignore_index=True)\n",
    "        dataset.to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "        if data[\"Choice\"].iloc[0] == \"Yes\":\n",
    "            best_message = candidate_message\n",
    "            best_response = data[\"Response1\"].iloc[0]\n",
    "            print(f\"New best message:{best_message}\")\n",
    "            if battles_generation == \"dense\":\n",
    "                generation_distance = 0\n",
    "\n",
    "        print(f\"Instruction1: {candidate_message[-1][\"content\"]}\")\n",
    "\n",
    "        if battles_generation == \"only-new\":\n",
    "            new_message = [{\"role\": candidate_message[0][\"role\"], \"content\":data[\"Response1\"].iloc[0]}]\n",
    "            if new_message not in next_candidate_messages:\n",
    "                next_candidate_messages.append(new_message)\n",
    "                if roles == \"system-user\":\n",
    "                    next_candidate_messages.append(switch_system_and_user(new_message))\n",
    "\n",
    "    if battles_generation == \"only-new\":\n",
    "        candidate_messages = next_candidate_messages\n",
    "        next_candidate_messages = []\n",
    "    if battles_generation == \"dense\":\n",
    "        #print(\"going to list candidate messages\")\n",
    "        candidate_messages = list_candidate_messages(dataset, best_message, generation_distance, roles)\n",
    "        generation_distance += 1\n",
    "        #print(\"candidate messages listed\")\n",
    "    print(f\"all done, generation distance: {generation_distance}, number of candidate messages: {len(candidate_messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in dataset[dataset[\"Choice\"]==\"Yes\"].iloc[-1].drop(\"Data\"):\n",
    "    display_markdown(entry, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Sampled\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdbe172e46cfba2329a5e8d5b64cdf2d12f4dfd7d9bcea153ecef62d1d51933b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
