{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BATTLE Eval\n",
    "\n",
    "This notebook shows how to:\n",
    "- Build and run an eval\n",
    "- Load the results and into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_message = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "battles_generation = \"dense\"      # methods: dense, only-new\n",
    "roles = \"system-user\"             # methods: system-user, system, user\n",
    "\n",
    "#!cd evals\n",
    "#!git lfs fetch --all\n",
    "#!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import yaml\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "#api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#client = OpenAI()\n",
    "\n",
    "# Install Evals if you haven't already\n",
    "# %pip install -e ../.\n",
    "# pip install --upgrade openai\n",
    "# %pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths. Assuming this notebook is in examples/\n",
    "\n",
    "evals_path = os.path.join(os.getcwd(), \"..\", \"evals\")\n",
    "\n",
    "registry_path = os.path.join(evals_path, \"registry\", \"evals\", \"battles.yaml\")\n",
    "\n",
    "data_path = os.path.join(evals_path, \"registry\", \"data\", \"battles\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "data_path = os.path.join(data_path, \"samples.jsonl\")\n",
    "\n",
    "json_logs_path = os.path.join(os.getcwd(), \"logs\")\n",
    "os.makedirs(json_logs_path, exist_ok=True)\n",
    "json_logs_path = os.path.join(json_logs_path, \"logs\")\n",
    "\n",
    "df_path = os.path.join(evals_path, \"evallogs\", \"df\")\n",
    "os.makedirs(df_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registry yaml\n",
    "\n",
    "registry_yaml = {}\n",
    "\n",
    "registry_yaml[\"battles\"] = {\n",
    "    \"id\": \"battles.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"battles.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.modelgraded.classify:ModelBasedClassify\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"battles/samples.jsonl\",\n",
    "        \"eval_type\": \"cot_classify\",\n",
    "        \"modelgraded_spec\": \"battle\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(registry_path), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "\n",
    "dataset = []\n",
    "candidate_messages = [initial_message]\n",
    "next_candidate_messages = []\n",
    "best_responses = []\n",
    "best_messages = [initial_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(best_message, candidate_message):\n",
    "\n",
    "    dataset = [{\"input1\": candidate_message, \"input2\": best_message}]\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(data_path, orient=\"records\", lines=True)\n",
    "\n",
    "    !oaieval gpt-3.5-turbo battles --record_path logs/logs\n",
    "    \n",
    "    with open(json_logs_path, \"r\") as f:\n",
    "        df = pd.read_json(f, lines=True)\n",
    "\n",
    "    df.to_json(os.path.join(df_path, \"events\"), lines=True, orient=\"records\")\n",
    "\n",
    "    instruction1 = candidate_message[0][\"content\"]\n",
    "    instruction2 = best_message[0][\"content\"]\n",
    "\n",
    "    battle_prompt_content = df[\"data\"].iloc[-2][\"prompt\"][0][\"content\"]\n",
    "    response1 = battle_prompt_content.split(\"\\n[Response 1]\\n\",)[1].split(\"\\n\\n[Instruction 2]\\n\")[0]\n",
    "    response1 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    response2 = battle_prompt_content.split(\"\\n[Response 2]\\n\",)[1].split(\"\\n\\n\\nIs the first response better than the second?\")[0]\n",
    "    response2 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    sampled = events_df[\"data\"].iloc[-2][\"sampled\"][0]\n",
    "\n",
    "    choice = events_df[\"data\"].iloc[-1][\"choice\"]\n",
    "\n",
    "    data = {'Instruction1': instruction1, 'Response1': response1, 'Instruction2': instruction2, 'Response2': response2, 'Sampled': sampled, 'Choice': choice, 'Data': df.to_dict()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_system_and_user(message):\n",
    "    new_message = []\n",
    "    if message[0][\"role\"] == \"system\":\n",
    "        new_message.append([{\"role\": \"user\", \"content\": message[0][\"content\"]}])\n",
    "    else:\n",
    "        new_message.append([{\"role\": \"system\", \"content\": message[0][\"content\"]}])\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_candidate_messages(dataset):\n",
    "    messages = []\n",
    "    list_of_contents = pd.DataFrame(dataset)[\"Response1\"].unique().tolist().insert(0,\"\")\n",
    "    for content in list_of_contents:\n",
    "        if roles == \"system-user\" or \"user\":\n",
    "            messages.append([{\"role\":\"user\",\"content\":content}])\n",
    "        if roles == \"system-user\" or \"system\":\n",
    "            messages.append([{\"role\":\"system\",\"content\":content}])\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-07 00:28:16,760] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/evals\n",
      "[2024-08-07 00:28:17,623] [registry.py:271] Loading registry from /Users/janvotava/.evals/evals\n",
      "[2024-08-07 00:28:17,625] [oaieval.py:215] \u001b[1;35mRun started: 240806222817YUGAB6S5\u001b[0m\n",
      "[2024-08-07 00:28:17,627] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/modelgraded\n",
      "[2024-08-07 00:28:17,654] [registry.py:271] Loading registry from /Users/janvotava/.evals/modelgraded\n",
      "[2024-08-07 00:28:17,654] [data.py:94] Fetching /Users/janvotava/Desktop/evals/evals/registry/data/battles/samples.jsonl\n",
      "[2024-08-07 00:28:17,654] [eval.py:36] Evaluating 1 samples\n",
      "[2024-08-07 00:28:17,675] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "[2024-08-07 00:28:21,253] [oaieval.py:275] Found 3/3 sampling events with usage data\n",
      "[2024-08-07 00:28:21,253] [oaieval.py:283] Token usage from 3 sampling events:\n",
      "completion_tokens: 103\n",
      "prompt_tokens: 201\n",
      "total_tokens: 304\n",
      "[2024-08-07 00:28:21,254] [record.py:371] Final report: {'counts/Yes': 1, 'score': 1.0, 'usage_completion_tokens': 103, 'usage_prompt_tokens': 201, 'usage_total_tokens': 304}. Logged to logs/logs\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:233] Final report:\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:235] counts/Yes: 1\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:235] score: 1.0\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:235] usage_completion_tokens: 103\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:235] usage_prompt_tokens: 201\n",
      "[2024-08-07 00:28:21,255] [oaieval.py:235] usage_total_tokens: 304\n",
      "[2024-08-07 00:28:21,259] [record.py:360] Logged 4 rows of events to logs/logs: insert_time=2.843ms\n",
      "Hello! I'm just here to chat and answer any questions you may have. How can I help you today?\n",
      "[2024-08-07 00:28:23,740] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/evals\n",
      "[2024-08-07 00:28:25,162] [registry.py:271] Loading registry from /Users/janvotava/.evals/evals\n",
      "[2024-08-07 00:28:25,163] [oaieval.py:215] \u001b[1;35mRun started: 240806222825QAKKB6DO\u001b[0m\n",
      "[2024-08-07 00:28:25,165] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/modelgraded\n",
      "[2024-08-07 00:28:25,205] [registry.py:271] Loading registry from /Users/janvotava/.evals/modelgraded\n",
      "[2024-08-07 00:28:25,205] [data.py:94] Fetching /Users/janvotava/Desktop/evals/evals/registry/data/battles/samples.jsonl\n",
      "[2024-08-07 00:28:25,206] [eval.py:36] Evaluating 1 samples\n",
      "[2024-08-07 00:28:25,237] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.29s/it]\n",
      "[2024-08-07 00:28:28,529] [oaieval.py:275] Found 3/3 sampling events with usage data\n",
      "[2024-08-07 00:28:28,529] [oaieval.py:283] Token usage from 3 sampling events:\n",
      "completion_tokens: 84\n",
      "prompt_tokens: 187\n",
      "total_tokens: 271\n",
      "[2024-08-07 00:28:28,530] [record.py:371] Final report: {'counts/No': 1, 'score': 0.0, 'usage_completion_tokens': 84, 'usage_prompt_tokens': 187, 'usage_total_tokens': 271}. Logged to logs/logs\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:233] Final report:\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:235] counts/No: 1\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:235] score: 0.0\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:235] usage_completion_tokens: 84\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:235] usage_prompt_tokens: 187\n",
      "[2024-08-07 00:28:28,530] [oaieval.py:235] usage_total_tokens: 271\n",
      "[2024-08-07 00:28:28,534] [record.py:360] Logged 4 rows of events to logs/logs: insert_time=2.260ms\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "for candidate_message in candidate_messages:\n",
    "    data = battle(best_messages[-1], candidate_message)\n",
    "    if data[\"Choice\"] == \"Yes\":\n",
    "        best_messages.append(candidate_message)\n",
    "        best_responses.append(data[\"Response1\"]) \n",
    "    dataset.append(data)\n",
    "    pd.DataFrame(dataset).to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "\n",
    "    if battles_generation == \"only-new\":\n",
    "        new_message = [{\"role\": candidate_message[0][\"role\"], \"content\":data[\"Response1\"]}]\n",
    "        if new_message not in next_candidate_messages:\n",
    "            next_candidate_messages.append(new_message)\n",
    "            if roles == \"system-user\":\n",
    "                next_candidate_messages.append(switch_system_and_user(new_message))\n",
    "\n",
    "if battles_generation == \"only-new\":\n",
    "    candidate_messages = next_candidate_messages\n",
    "    next_candidate_messages = []\n",
    "if battles_generation == \"dense\":\n",
    "    candidate_messages = list_candidate_messages(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user', 'content': ''}]]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion = client.chat.completions.create(\n",
    "    #messages = [{\"role\":\"system\", \"content\":\"Hello! How can I assist you today?\"}],\n",
    "    #model = \"gpt-3.5-turbo\",\n",
    "    #temperature = 0,\n",
    "    #seed = 20220722\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Instruction1': '',\n",
       "  'Response1': 'Hello! How can I assist you today?',\n",
       "  'Instruction2': '',\n",
       "  'Response2': 'Hello! How can I assist you today?',\n",
       "  'Sampled': '1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses effectively address the instruction given.\\n\\nNo\\n\\nNo',\n",
       "  'Choice': 'No',\n",
       "  'Data': {'spec': {0: {'completion_fns': ['gpt-3.5-turbo'],\n",
       "     'eval_name': 'battles.test.v1',\n",
       "     'base_eval': 'battles',\n",
       "     'split': 'test',\n",
       "     'run_config': {'completion_fns': ['gpt-3.5-turbo'],\n",
       "      'eval_spec': {'cls': 'evals.elsuite.modelgraded.classify:ModelBasedClassify',\n",
       "       'registry_path': '/Users/janvotava/Desktop/evals/evals/registry',\n",
       "       'args': {'eval_type': 'cot_classify',\n",
       "        'modelgraded_spec': 'battle',\n",
       "        'samples_jsonl': 'battles/samples.jsonl'},\n",
       "       'key': 'battles.test.v1',\n",
       "       'group': 'battles'},\n",
       "      'seed': 20220722,\n",
       "      'max_samples': None,\n",
       "      'command': '/opt/anaconda3/bin/oaieval gpt-3.5-turbo battles --record_path logs/logs',\n",
       "      'initial_settings': {'visible': True}},\n",
       "     'created_by': '',\n",
       "     'run_id': '2408062227417TNSSWPN',\n",
       "     'created_at': '2024-08-06 22:27:41.450703'},\n",
       "    1: nan,\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'final_report': {0: nan,\n",
       "    1: {'counts/No': 1,\n",
       "     'score': 0.0,\n",
       "     'usage_completion_tokens': 82,\n",
       "     'usage_prompt_tokens': 169,\n",
       "     'usage_total_tokens': 251},\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'run_id': {0: nan,\n",
       "    1: '2408062227417TNSSWPN',\n",
       "    2: '2408062227417TNSSWPN',\n",
       "    3: '2408062227417TNSSWPN',\n",
       "    4: '2408062227417TNSSWPN',\n",
       "    5: '2408062227417TNSSWPN'},\n",
       "   'event_id': {0: nan, 1: nan, 2: 0.0, 3: 1.0, 4: 2.0, 5: 3.0},\n",
       "   'sample_id': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'battles.test.0',\n",
       "    3: 'battles.test.0',\n",
       "    4: 'battles.test.0',\n",
       "    5: 'battles.test.0'},\n",
       "   'type': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'sampling',\n",
       "    3: 'sampling',\n",
       "    4: 'sampling',\n",
       "    5: 'metrics'},\n",
       "   'data': {0: nan,\n",
       "    1: nan,\n",
       "    2: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    3: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    4: {'prompt': [{'role': 'user',\n",
       "       'content': 'You are comparing two responses to the following two instructions.\\n\\n[Instruction 1]\\n\\n[Response 1]\\nHello! How can I assist you today?\\n\\n[Instruction 2]\\n\\n[Response 2]\\nHello! How can I assist you today?\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \"Yes\" or \"No\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:'}],\n",
       "     'sampled': ['1. Both responses are identical in content and structure.\\n2. The only difference between the two responses is the numbering (1 vs 2) in the instruction.\\n3. The numbering does not affect the quality or effectiveness of the response.\\n4. Therefore, the first response is not better than the second response.\\n\\nNo'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 64,\n",
       "      'prompt_tokens': 155,\n",
       "      'total_tokens': 219}},\n",
       "    5: {'choice': 'No', 'score': 0.0}},\n",
       "   'created_by': {0: nan, 1: nan, 2: '', 3: '', 4: '', 5: ''},\n",
       "   'created_at': {0: NaT,\n",
       "    1: NaT,\n",
       "    2: Timestamp('2024-08-06 22:27:42.985376+0000', tz='UTC'),\n",
       "    3: Timestamp('2024-08-06 22:27:43.643879+0000', tz='UTC'),\n",
       "    4: Timestamp('2024-08-06 22:27:45.310034+0000', tz='UTC'),\n",
       "    5: Timestamp('2024-08-06 22:27:45.310140+0000', tz='UTC')}}}]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdbe172e46cfba2329a5e8d5b64cdf2d12f4dfd7d9bcea153ecef62d1d51933b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
