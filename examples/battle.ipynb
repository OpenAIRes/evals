{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BATTLE Eval\n",
    "\n",
    "This notebook shows how to:\n",
    "- Build and run an eval\n",
    "- Load the results and into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_message = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "battles_generation = \"dense\"      # methods: dense, only-new\n",
    "roles = \"system-user\"             # methods: system-user, system, user\n",
    "\n",
    "#!cd evals\n",
    "#!git lfs fetch --all\n",
    "#!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import yaml\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "#api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#client = OpenAI()\n",
    "\n",
    "# Install Evals if you haven't already\n",
    "# %pip install -e ../.\n",
    "# pip install --upgrade openai\n",
    "# %pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths. Assuming this notebook is in examples/\n",
    "\n",
    "evals_path = os.path.join(os.getcwd(), \"..\", \"evals\")\n",
    "\n",
    "registry_path = os.path.join(evals_path, \"registry\", \"evals\", \"battles.yaml\")\n",
    "\n",
    "data_path = os.path.join(evals_path, \"registry\", \"data\", \"battles\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "data_path = os.path.join(data_path, \"samples.jsonl\")\n",
    "\n",
    "json_logs_path = os.path.join(os.getcwd(), \"logs\")\n",
    "os.makedirs(json_logs_path, exist_ok=True)\n",
    "json_logs_path = os.path.join(json_logs_path, \"logs\")\n",
    "\n",
    "df_path = os.path.join(evals_path, \"evallogs\", \"df\")\n",
    "os.makedirs(df_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registry yaml\n",
    "\n",
    "registry_yaml = {}\n",
    "\n",
    "registry_yaml[\"battles\"] = {\n",
    "    \"id\": \"battles.test.v1\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "registry_yaml[\"battles.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.modelgraded.classify:ModelBasedClassify\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": \"battles/samples.jsonl\",\n",
    "        \"eval_type\": \"cot_classify\",\n",
    "        \"modelgraded_spec\": \"battle\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(registry_path), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "\n",
    "dataset = []\n",
    "candidate_messages = [initial_message]\n",
    "next_candidate_messages = []\n",
    "best_responses = []\n",
    "best_messages = [initial_message]\n",
    "generation_distance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(best_message, candidate_message):\n",
    "\n",
    "    dataset = [{\"input1\": candidate_message, \"input2\": best_message}]\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_json(data_path, orient=\"records\", lines=True)\n",
    "\n",
    "    !oaieval gpt-3.5-turbo battles --record_path logs/logs\n",
    "    \n",
    "    with open(json_logs_path, \"r\") as f:\n",
    "        df = pd.read_json(f, lines=True)\n",
    "\n",
    "    df.to_json(os.path.join(df_path, \"events\"), lines=True, orient=\"records\")\n",
    "\n",
    "    instruction1 = candidate_message[0][\"content\"]\n",
    "    instruction2 = best_message[0][\"content\"]\n",
    "\n",
    "    battle_prompt_content = df[\"data\"].iloc[-2][\"prompt\"][0][\"content\"]\n",
    "    response1 = battle_prompt_content.split(\"\\n[Response 1]\\n\",)[1].split(\"\\n\\n[Instruction 2]\\n\")[0]\n",
    "    response1 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "    response2 = battle_prompt_content.split(\"\\n[Response 2]\\n\",)[1].split(\"\\n\\n\\nIs the first response better than the second?\")[0]\n",
    "    response2 = response1.replace(\"\\\\'\", \"'\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    sampled = events_df[\"data\"].iloc[-2][\"sampled\"][0]\n",
    "\n",
    "    choice = events_df[\"data\"].iloc[-1][\"choice\"]\n",
    "\n",
    "    data = {'Instruction1': instruction1, 'Response1': response1, 'Instruction2': instruction2, 'Response2': response2, 'Sampled': sampled, 'Choice': choice, 'Data': df.to_dict()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_system_and_user(message):\n",
    "    new_message = []\n",
    "    if message[0][\"role\"] == \"system\":\n",
    "        new_message.append([{\"role\": \"user\", \"content\": message[0][\"content\"]}])\n",
    "    else:\n",
    "        new_message.append([{\"role\": \"system\", \"content\": message[0][\"content\"]}])\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(content, dataset):\n",
    "    return pd.DataFrame(dataset).loc[dataset[\"Response1\"] == content][\"Instruction1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_children(content, dataset):\n",
    "    return pd.DataFrame(dataset).loc[dataset[\"Instruction1\"] == content][\"Response1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_candidate_messages(dataset, best_message, generation_distance):\n",
    "    best_content = best_message[0][\"content\"]\n",
    "    last_up = [best_content]\n",
    "    last_down = [best_content]\n",
    "    list_of_contents = [best_content]\n",
    "\n",
    "    i = 0\n",
    "    next_up = []\n",
    "    next_down = []\n",
    "    while i < generation_distance:\n",
    "        for content in last_up:\n",
    "            next_up.extend(find_parents(content, dataset))\n",
    "        for content in last_down:\n",
    "            next_down.extend(find_children(content, dataset))\n",
    "        list_of_contents.extend(next_up)\n",
    "        list_of_contents.extend(next_down)\n",
    "        last_up = next_up\n",
    "        last_down = next_down\n",
    "        i += 1\n",
    "\n",
    "    list_of_contents = pd.Array(list_of_contents).unique().tolist()\n",
    "    \n",
    "    #best_message_index = list_of_contents.index(best_message[0][content])\n",
    "    #start = max(best_message_index - generation_distance, 0)\n",
    "    #stop = min(len(list_of_contents))\n",
    "    #stop = len(list_of_contents) - 1\n",
    "\n",
    "    for content in list_of_contents:\n",
    "        if roles == \"system-user\" or \"user\":\n",
    "            messages.append([{\"role\":\"user\",\"content\":content}])\n",
    "        if roles == \"system-user\" or \"system\":\n",
    "            messages.append([{\"role\":\"system\",\"content\":content}])\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': ''}]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_messages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Instruction1': '',\n",
       "  'Response1': 'Hello! How can I assist you today?',\n",
       "  'Instruction2': '',\n",
       "  'Response2': 'Hello! How can I assist you today?',\n",
       "  'Sampled': '1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses effectively address the instruction given.\\n\\nNo\\n\\nNo',\n",
       "  'Choice': 'No',\n",
       "  'Data': {'spec': {0: {'completion_fns': ['gpt-3.5-turbo'],\n",
       "     'eval_name': 'battles.test.v1',\n",
       "     'base_eval': 'battles',\n",
       "     'split': 'test',\n",
       "     'run_config': {'completion_fns': ['gpt-3.5-turbo'],\n",
       "      'eval_spec': {'cls': 'evals.elsuite.modelgraded.classify:ModelBasedClassify',\n",
       "       'registry_path': '/Users/janvotava/Desktop/evals/evals/registry',\n",
       "       'args': {'eval_type': 'cot_classify',\n",
       "        'modelgraded_spec': 'battle',\n",
       "        'samples_jsonl': 'battles/samples.jsonl'},\n",
       "       'key': 'battles.test.v1',\n",
       "       'group': 'battles'},\n",
       "      'seed': 20220722,\n",
       "      'max_samples': None,\n",
       "      'command': '/opt/anaconda3/bin/oaieval gpt-3.5-turbo battles --record_path logs/logs',\n",
       "      'initial_settings': {'visible': True}},\n",
       "     'created_by': '',\n",
       "     'run_id': '240807125145CEDHETSF',\n",
       "     'created_at': '2024-08-07 12:51:45.563403'},\n",
       "    1: nan,\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'final_report': {0: nan,\n",
       "    1: {'counts/No': 1,\n",
       "     'score': 0.0,\n",
       "     'usage_completion_tokens': 56,\n",
       "     'usage_prompt_tokens': 169,\n",
       "     'usage_total_tokens': 225},\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'run_id': {0: nan,\n",
       "    1: '240807125145CEDHETSF',\n",
       "    2: '240807125145CEDHETSF',\n",
       "    3: '240807125145CEDHETSF',\n",
       "    4: '240807125145CEDHETSF',\n",
       "    5: '240807125145CEDHETSF'},\n",
       "   'event_id': {0: nan, 1: nan, 2: 0.0, 3: 1.0, 4: 2.0, 5: 3.0},\n",
       "   'sample_id': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'battles.test.0',\n",
       "    3: 'battles.test.0',\n",
       "    4: 'battles.test.0',\n",
       "    5: 'battles.test.0'},\n",
       "   'type': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'sampling',\n",
       "    3: 'sampling',\n",
       "    4: 'sampling',\n",
       "    5: 'metrics'},\n",
       "   'data': {0: nan,\n",
       "    1: nan,\n",
       "    2: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    3: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    4: {'prompt': [{'role': 'user',\n",
       "       'content': 'You are comparing two responses to the following two instructions.\\n\\n[Instruction 1]\\n\\n[Response 1]\\nHello! How can I assist you today?\\n\\n[Instruction 2]\\n\\n[Response 2]\\nHello! How can I assist you today?\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \"Yes\" or \"No\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:'}],\n",
       "     'sampled': ['1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses follow the same format and tone.\\n\\nNo\\n\\nNo'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 38,\n",
       "      'prompt_tokens': 155,\n",
       "      'total_tokens': 193}},\n",
       "    5: {'choice': 'No', 'score': 0.0}},\n",
       "   'created_by': {0: nan, 1: nan, 2: '', 3: '', 4: '', 5: ''},\n",
       "   'created_at': {0: NaT,\n",
       "    1: NaT,\n",
       "    2: Timestamp('2024-08-07 12:51:47.144713+0000', tz='UTC'),\n",
       "    3: Timestamp('2024-08-07 12:51:48.160947+0000', tz='UTC'),\n",
       "    4: Timestamp('2024-08-07 12:51:49.386215+0000', tz='UTC'),\n",
       "    5: Timestamp('2024-08-07 12:51:49.386288+0000', tz='UTC')}}}]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252241.23s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "[2024-08-07 15:20:05,358] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/evals\n",
      "[2024-08-07 15:20:06,354] [registry.py:271] Loading registry from /Users/janvotava/.evals/evals\n",
      "[2024-08-07 15:20:06,357] [oaieval.py:215] \u001b[1;35mRun started: 2408071320067FKIOHKH\u001b[0m\n",
      "[2024-08-07 15:20:06,359] [registry.py:271] Loading registry from /Users/janvotava/Desktop/evals/evals/registry/modelgraded\n",
      "[2024-08-07 15:20:06,394] [registry.py:271] Loading registry from /Users/janvotava/.evals/modelgraded\n",
      "[2024-08-07 15:20:06,395] [data.py:94] Fetching /Users/janvotava/Desktop/evals/evals/registry/data/battles/samples.jsonl\n",
      "[2024-08-07 15:20:06,396] [eval.py:36] Evaluating 1 samples\n",
      "[2024-08-07 15:20:06,441] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.65s/it]\n",
      "[2024-08-07 15:20:11,092] [oaieval.py:275] Found 3/3 sampling events with usage data\n",
      "[2024-08-07 15:20:11,092] [oaieval.py:283] Token usage from 3 sampling events:\n",
      "completion_tokens: 79\n",
      "prompt_tokens: 169\n",
      "total_tokens: 248\n",
      "[2024-08-07 15:20:11,096] [record.py:371] Final report: {'counts/No': 1, 'score': 0.0, 'usage_completion_tokens': 79, 'usage_prompt_tokens': 169, 'usage_total_tokens': 248}. Logged to logs/logs\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:233] Final report:\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:235] counts/No: 1\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:235] score: 0.0\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:235] usage_completion_tokens: 79\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:235] usage_prompt_tokens: 169\n",
      "[2024-08-07 15:20:11,096] [oaieval.py:235] usage_total_tokens: 248\n",
      "[2024-08-07 15:20:11,113] [record.py:360] Logged 4 rows of events to logs/logs: insert_time=15.191ms\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[445], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     next_candidate_messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m battles_generation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     candidate_messages \u001b[38;5;241m=\u001b[39m list_candidate_messages(dataset, best_messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], generation_distance)\n\u001b[1;32m     23\u001b[0m     generation_distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[440], line 12\u001b[0m, in \u001b[0;36mlist_candidate_messages\u001b[0;34m(dataset, best_message, generation_distance)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m generation_distance:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m last_up:\n\u001b[0;32m---> 12\u001b[0m         next_up\u001b[38;5;241m.\u001b[39mextend(find_parents(content, dataset))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m last_down:\n\u001b[1;32m     14\u001b[0m         next_down\u001b[38;5;241m.\u001b[39mextend(find_children(content, dataset))\n",
      "Cell \u001b[0;32mIn[438], line 2\u001b[0m, in \u001b[0;36mfind_parents\u001b[0;34m(content, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_parents\u001b[39m(content, dataset):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataset)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m content][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruction1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "for candidate_message in candidate_messages:\n",
    "    data = battle(best_messages[-1], candidate_message)\n",
    "    dataset.append(data)\n",
    "    pd.DataFrame(dataset).to_json(os.path.join(df_path, \"dataset\"), lines=True, orient=\"records\")\n",
    "    if data[\"Choice\"] == \"Yes\":\n",
    "        best_messages.append(candidate_message)\n",
    "        best_responses.append(data[\"Response1\"])\n",
    "        if battles_generation == \"dense\":\n",
    "            generation_distance = 0\n",
    "\n",
    "    if battles_generation == \"only-new\":\n",
    "        new_message = [{\"role\": candidate_message[0][\"role\"], \"content\":data[\"Response1\"]}]\n",
    "        if new_message not in next_candidate_messages:\n",
    "            next_candidate_messages.append(new_message)\n",
    "            if roles == \"system-user\":\n",
    "                next_candidate_messages.append(switch_system_and_user(new_message))\n",
    "\n",
    "if battles_generation == \"only-new\":\n",
    "    candidate_messages = next_candidate_messages\n",
    "    next_candidate_messages = []\n",
    "if battles_generation == \"dense\":\n",
    "    candidate_messages = list_candidate_messages(dataset, best_messages[-1], generation_distance)\n",
    "    generation_distance += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion = client.chat.completions.create(\n",
    "    #messages = [{\"role\":\"system\", \"content\":\"Hello! How can I assist you today?\"}],\n",
    "    #model = \"gpt-3.5-turbo\",\n",
    "    #temperature = 0,\n",
    "    #seed = 20220722\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m generation_distance:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m last_up:\n\u001b[0;32m---> 11\u001b[0m         next_up\u001b[38;5;241m.\u001b[39mextend(find_parents(content, dataset))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m last_down:\n\u001b[1;32m     13\u001b[0m         next_down\u001b[38;5;241m.\u001b[39mextend(find_children(content, dataset))\n",
      "Cell \u001b[0;32mIn[438], line 2\u001b[0m, in \u001b[0;36mfind_parents\u001b[0;34m(content, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_parents\u001b[39m(content, dataset):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataset)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m content][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruction1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "best_content = best_message[0][\"content\"]\n",
    "last_up = [best_content]\n",
    "last_down = [best_content]\n",
    "list_of_contents = [best_content]\n",
    "\n",
    "i = 0\n",
    "next_up = []\n",
    "next_down = []\n",
    "while i < generation_distance:\n",
    "    for content in last_up:\n",
    "        next_up.extend(find_parents(content, dataset))\n",
    "    for content in last_down:\n",
    "        next_down.extend(find_children(content, dataset))\n",
    "    list_of_contents.extend(next_up)\n",
    "    list_of_contents.extend(next_down)\n",
    "    last_up = next_up\n",
    "    last_down = next_down\n",
    "    i += 1\n",
    "\n",
    "list_of_contents = pd.Array(list_of_contents).unique().tolist()\n",
    "\n",
    "#best_message_index = list_of_contents.index(best_message[0][content])\n",
    "#start = max(best_message_index - generation_distance, 0)\n",
    "#stop = min(len(list_of_contents))\n",
    "#stop = len(list_of_contents) - 1\n",
    "\n",
    "for content in list_of_contents:\n",
    "    if roles == \"system-user\" or \"user\":\n",
    "        messages.append([{\"role\":\"user\",\"content\":content}])\n",
    "    if roles == \"system-user\" or \"system\":\n",
    "        messages.append([{\"role\":\"system\",\"content\":content}])\n",
    "return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Instruction1': '',\n",
       "  'Response1': 'Hello! How can I assist you today?',\n",
       "  'Instruction2': '',\n",
       "  'Response2': 'Hello! How can I assist you today?',\n",
       "  'Sampled': '1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses effectively address the instruction given.\\n\\nNo\\n\\nNo',\n",
       "  'Choice': 'No',\n",
       "  'Data': {'spec': {0: {'completion_fns': ['gpt-3.5-turbo'],\n",
       "     'eval_name': 'battles.test.v1',\n",
       "     'base_eval': 'battles',\n",
       "     'split': 'test',\n",
       "     'run_config': {'completion_fns': ['gpt-3.5-turbo'],\n",
       "      'eval_spec': {'cls': 'evals.elsuite.modelgraded.classify:ModelBasedClassify',\n",
       "       'registry_path': '/Users/janvotava/Desktop/evals/evals/registry',\n",
       "       'args': {'eval_type': 'cot_classify',\n",
       "        'modelgraded_spec': 'battle',\n",
       "        'samples_jsonl': 'battles/samples.jsonl'},\n",
       "       'key': 'battles.test.v1',\n",
       "       'group': 'battles'},\n",
       "      'seed': 20220722,\n",
       "      'max_samples': None,\n",
       "      'command': '/opt/anaconda3/bin/oaieval gpt-3.5-turbo battles --record_path logs/logs',\n",
       "      'initial_settings': {'visible': True}},\n",
       "     'created_by': '',\n",
       "     'run_id': '240807125145CEDHETSF',\n",
       "     'created_at': '2024-08-07 12:51:45.563403'},\n",
       "    1: nan,\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'final_report': {0: nan,\n",
       "    1: {'counts/No': 1,\n",
       "     'score': 0.0,\n",
       "     'usage_completion_tokens': 56,\n",
       "     'usage_prompt_tokens': 169,\n",
       "     'usage_total_tokens': 225},\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'run_id': {0: nan,\n",
       "    1: '240807125145CEDHETSF',\n",
       "    2: '240807125145CEDHETSF',\n",
       "    3: '240807125145CEDHETSF',\n",
       "    4: '240807125145CEDHETSF',\n",
       "    5: '240807125145CEDHETSF'},\n",
       "   'event_id': {0: nan, 1: nan, 2: 0.0, 3: 1.0, 4: 2.0, 5: 3.0},\n",
       "   'sample_id': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'battles.test.0',\n",
       "    3: 'battles.test.0',\n",
       "    4: 'battles.test.0',\n",
       "    5: 'battles.test.0'},\n",
       "   'type': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'sampling',\n",
       "    3: 'sampling',\n",
       "    4: 'sampling',\n",
       "    5: 'metrics'},\n",
       "   'data': {0: nan,\n",
       "    1: nan,\n",
       "    2: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    3: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    4: {'prompt': [{'role': 'user',\n",
       "       'content': 'You are comparing two responses to the following two instructions.\\n\\n[Instruction 1]\\n\\n[Response 1]\\nHello! How can I assist you today?\\n\\n[Instruction 2]\\n\\n[Response 2]\\nHello! How can I assist you today?\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \"Yes\" or \"No\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:'}],\n",
       "     'sampled': ['1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses follow the same format and tone.\\n\\nNo\\n\\nNo'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 38,\n",
       "      'prompt_tokens': 155,\n",
       "      'total_tokens': 193}},\n",
       "    5: {'choice': 'No', 'score': 0.0}},\n",
       "   'created_by': {0: nan, 1: nan, 2: '', 3: '', 4: '', 5: ''},\n",
       "   'created_at': {0: NaT,\n",
       "    1: NaT,\n",
       "    2: Timestamp('2024-08-07 12:51:47.144713+0000', tz='UTC'),\n",
       "    3: Timestamp('2024-08-07 12:51:48.160947+0000', tz='UTC'),\n",
       "    4: Timestamp('2024-08-07 12:51:49.386215+0000', tz='UTC'),\n",
       "    5: Timestamp('2024-08-07 12:51:49.386288+0000', tz='UTC')}}},\n",
       " {'Instruction1': '',\n",
       "  'Response1': 'Hello! How can I assist you today?',\n",
       "  'Instruction2': '',\n",
       "  'Response2': 'Hello! How can I assist you today?',\n",
       "  'Sampled': '1. Both responses are identical in content and structure.\\n2. There is no distinguishable difference between the two responses.\\n3. Both responses effectively address the instruction given.\\n\\nNo\\n\\nNo',\n",
       "  'Choice': 'No',\n",
       "  'Data': {'spec': {0: {'completion_fns': ['gpt-3.5-turbo'],\n",
       "     'eval_name': 'battles.test.v1',\n",
       "     'base_eval': 'battles',\n",
       "     'split': 'test',\n",
       "     'run_config': {'completion_fns': ['gpt-3.5-turbo'],\n",
       "      'eval_spec': {'cls': 'evals.elsuite.modelgraded.classify:ModelBasedClassify',\n",
       "       'registry_path': '/Users/janvotava/Desktop/evals/evals/registry',\n",
       "       'args': {'eval_type': 'cot_classify',\n",
       "        'modelgraded_spec': 'battle',\n",
       "        'samples_jsonl': 'battles/samples.jsonl'},\n",
       "       'key': 'battles.test.v1',\n",
       "       'group': 'battles'},\n",
       "      'seed': 20220722,\n",
       "      'max_samples': None,\n",
       "      'command': '/opt/anaconda3/bin/oaieval gpt-3.5-turbo battles --record_path logs/logs',\n",
       "      'initial_settings': {'visible': True}},\n",
       "     'created_by': '',\n",
       "     'run_id': '2408071320067FKIOHKH',\n",
       "     'created_at': '2024-08-07 13:20:06.355129'},\n",
       "    1: nan,\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'final_report': {0: nan,\n",
       "    1: {'counts/No': 1,\n",
       "     'score': 0.0,\n",
       "     'usage_completion_tokens': 79,\n",
       "     'usage_prompt_tokens': 169,\n",
       "     'usage_total_tokens': 248},\n",
       "    2: nan,\n",
       "    3: nan,\n",
       "    4: nan,\n",
       "    5: nan},\n",
       "   'run_id': {0: nan,\n",
       "    1: '2408071320067FKIOHKH',\n",
       "    2: '2408071320067FKIOHKH',\n",
       "    3: '2408071320067FKIOHKH',\n",
       "    4: '2408071320067FKIOHKH',\n",
       "    5: '2408071320067FKIOHKH'},\n",
       "   'event_id': {0: nan, 1: nan, 2: 0.0, 3: 1.0, 4: 2.0, 5: 3.0},\n",
       "   'sample_id': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'battles.test.0',\n",
       "    3: 'battles.test.0',\n",
       "    4: 'battles.test.0',\n",
       "    5: 'battles.test.0'},\n",
       "   'type': {0: nan,\n",
       "    1: nan,\n",
       "    2: 'sampling',\n",
       "    3: 'sampling',\n",
       "    4: 'sampling',\n",
       "    5: 'metrics'},\n",
       "   'data': {0: nan,\n",
       "    1: nan,\n",
       "    2: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    3: {'prompt': [{'role': 'user', 'content': ''}],\n",
       "     'sampled': ['Hello! How can I assist you today?'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 9,\n",
       "      'prompt_tokens': 7,\n",
       "      'total_tokens': 16}},\n",
       "    4: {'prompt': [{'role': 'user',\n",
       "       'content': 'You are comparing two responses to the following two instructions.\\n\\n[Instruction 1]\\n\\n[Response 1]\\nHello! How can I assist you today?\\n\\n[Instruction 2]\\n\\n[Response 2]\\nHello! How can I assist you today?\\n\\n\\nIs the first response better than the second? You must provide one answer based on your subjective view.\\n\\nFirst, write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Then print only a single choice from \"Yes\" or \"No\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the answer by itself on a new line.\\n\\nReasoning:'}],\n",
       "     'sampled': ['1. Both responses are identical in content and structure.\\n2. The only difference between the two responses is the numbering (1 vs 2) in the instruction.\\n3. Since the responses are the same, the numbering in the instruction does not affect the quality or effectiveness of the response.\\n\\nNo\\n\\nNo'],\n",
       "     'model': 'gpt-3.5-turbo-0125',\n",
       "     'usage': {'completion_tokens': 61,\n",
       "      'prompt_tokens': 155,\n",
       "      'total_tokens': 216}},\n",
       "    5: {'choice': 'No', 'score': 0.0}},\n",
       "   'created_by': {0: nan, 1: nan, 2: '', 3: '', 4: '', 5: ''},\n",
       "   'created_at': {0: NaT,\n",
       "    1: NaT,\n",
       "    2: Timestamp('2024-08-07 13:20:07.620966+0000', tz='UTC'),\n",
       "    3: Timestamp('2024-08-07 13:20:08.645184+0000', tz='UTC'),\n",
       "    4: Timestamp('2024-08-07 13:20:11.089385+0000', tz='UTC'),\n",
       "    5: Timestamp('2024-08-07 13:20:11.089466+0000', tz='UTC')}}}]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instruction1</th>\n",
       "      <th>Response1</th>\n",
       "      <th>Instruction2</th>\n",
       "      <th>Response2</th>\n",
       "      <th>Sampled</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Instruction1, Response1, Instruction2, Response2, Sampled, Choice, Data]\n",
       "Index: []"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test[\"Response1\"] == content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[447], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataset)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m==\u001b[39m content][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruction1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(dataset)[\"Instruction1\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdbe172e46cfba2329a5e8d5b64cdf2d12f4dfd7d9bcea153ecef62d1d51933b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
